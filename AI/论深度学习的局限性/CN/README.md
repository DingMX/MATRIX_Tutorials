# 论深度学习的局限性
这篇文章清晰地展现了深度学习本身真正的意义，让我们更透彻地了解它的同时，也明白了它的局限之处，距离着人类层次的 AI 还有着太长的路要走。

## 深度学习：几何角度

深度学习最令人惊讶的地方在于它的简单程度。十年之前，没有人会想到我们能够只是运用梯度下降算法训练简单的参数模型，就能在机器感知方面取得如此巨大的成果。现在来看，你只需要通过梯度下降方法，用 足够多 的样例去训练一个参数 足够多 的参数模型（就可以取得你想要的结果了）。就如 Feynman 曾经对宇宙的描述一样，“它并非复杂，只是数量巨大”

在深度学习中，所有东西都是一个向量，换言之，所有东西都是几何空间中的一个点。输入的模型（可以是文本，图像等等）和目标都会首先被“向量化”，也就是说，转化为初始的输入向量空间和目标向量空间。深度学习模型的每一层都对通过的数据，进行着一次简单的几何变换。而合并在一起，链在一起的各层模型形成了一个非常复杂的几何变换，分解成单一的之后又变的非常简单。这个复杂的变换试图将输入映射到输出，每次处理一个点。这个变换被各神经层的权重参数化，而权重则在每次迭代时基于当前模型的运行状况进行更新。这个几何变换的关键特征就是，它必须可导（可微），这样我们才能够通过梯度下降算法学习它的参数。直观地看，这意味着从输入到输出的几何变换必须是平稳且连续的 —— 这是一个重要的限制条件。

对于输入的复杂几何变换过程，可以在 3D 下画出一个人，在尝试平整一张卷成球的纸，来达到视觉化的目的：皱起来的纸球代表着模型中的输入副本。每次人对纸的动作都与每次单个神经层的简单几何变换相似。这样看，平整纸球的一套动作就是整个模型的几何变换，而当这些动作（几何变换）连在一起时，看起来会非常复杂。深度学习模型实际上是一个数学机器，将多种高维数据平整化。

这就是深度学习的神奇之处：将“意义”转变为向量到几何空间中，并逐渐地学习复杂的几何变换，将一个空间映射到另一个。你只需要有足够维度的空间，来获取原始数据中的所有存在的关系。
