# 论深度学习的局限性
这篇文章清晰地展现了深度学习本身真正的意义，让我们更透彻地了解它的同时，也明白了它的局限之处，距离着人类层次的 AI 还有着太长的路要走。

## 深度学习：几何角度

深度学习最令人惊讶的地方在于它的简单程度。十年之前，没有人会想到我们能够只是运用梯度下降算法训练简单的参数模型，就能在机器感知方面取得如此巨大的成果。现在来看，你只需要通过梯度下降方法，用 足够多 的样例去训练一个参数 足够多 的参数模型（就可以取得你想要的结果了）。就如 Feynman 曾经对宇宙的描述一样，“它并非复杂，只是数量巨大”

在深度学习中，所有东西都是一个向量，换言之，所有东西都是几何空间中的一个点。输入的模型（可以是文本，图像等等）和目标都会首先被“向量化”，也就是说，转化为初始的输入向量空间和目标向量空间。深度学习模型的每一层都对通过的数据，进行着一次简单的几何变换。而合并在一起，链在一起的各层模型形成了一个非常复杂的几何变换，分解成单一的之后又变的非常简单。这个复杂的变换试图将输入映射到输出，每次处理一个点。这个变换被各神经层的权重参数化，而权重则在每次迭代时基于当前模型的运行状况进行更新。这个几何变换的关键特征就是，它必须可导（可微），这样我们才能够通过梯度下降算法学习它的参数。直观地看，这意味着从输入到输出的几何变换必须是平稳且连续的 —— 这是一个重要的限制条件。

对于输入的复杂几何变换过程，可以在 3D 下画出一个人，在尝试平整一张卷成球的纸，来达到视觉化的目的：皱起来的纸球代表着模型中的输入副本。每次人对纸的动作都与每次单个神经层的简单几何变换相似。这样看，平整纸球的一套动作就是整个模型的几何变换，而当这些动作（几何变换）连在一起时，看起来会非常复杂。深度学习模型实际上是一个数学机器，将多种高维数据平整化。

这就是深度学习的神奇之处：将“意义”转变为向量到几何空间中，并逐渐地学习复杂的几何变换，将一个空间映射到另一个。你只需要有足够维度的空间，来获取原始数据中的所有存在的关系。


## 深度学习的局限性

我们可以将这个简单的策略应用到各个领域。不过，也有很多领域不在当前深度学习所能达到的范围，即使给它大量的人工注释过的数据。例如，你可以对一个软件的特征进行成百上千次不同的描述，像一个项目经理那样写，同时包含相对应的一组软件工程师开发出来的源代码来满足这些需求。即使有了这些数据，你也不能训练出一个深度学习模型来简单地阅读产品描述并生成一个正确的代码库。而这只是众多例子中的一个。总体上来讲，任何需要推理类的编程，科学的长时期规划，算法类的数据操作都处在深度学习模型之外，不论你给予模型多少数据都没用。即使让深度神经网络学习一个排序算法也是非常困难的。

这是因为一个深度学习模型“只是”一系列简单、连续的几何变换，将一个几何空间映射到另一个。假设 X 到 Y 存在着一个可学习的连续变换，同时有足够密集的 X:Y 的训练数据作为样例，它所能做的一切就是将数据副本 X 映射到另一个副本 Y。所以尽管一个深度学习模型可以被看作是一种程序，反过来大部分程序并不能表示成深度学习模型 —— 对于大多数任务，要么实际上不存在相关的深度学习模型来解决这种任务，或者即使这里存在一个，可能也是不可学习的，也就是说，相关的几何变换可能过于复杂，或者这里可能没有合适的数据来进行学习。
通过增加更多的神经层和使用更多的训练数据，来扩大当前的深度神经网络的规模，只能在一些问题中取得一定的进步。这种方法并不能够解决更多的基础性问题，那些问题在深度学习模型的能力之外，它们无法被表示，并且唯一的学习途径又不能够被表示对一个数据副本的连续几何变换。

## 将机器学习模型人格化的风险

一个目前 AI 领域非常现实的问题，就是错误地阐释深度学习模型的职能，并高估了它们的能力。人类意识的一个基本特征就是“理论思维”，我们倾向于将意图、信仰和知识投影在我们周围的东西上。在一个石头上画一个笑脸能让它“快乐”起来 —— 在我们的意识中。应用在深度学习中，这意味着当我们能够成功地训练出一个可以添加标题描述图像的模型时，我们会相信那个模型理解了图片的内容，同时也理解所生成的标题。接着，我们会对模型因为任何轻微的异常于训练数据的图片而生成的荒谬的标题感到惊讶。

![](https://user-gold-cdn.xitu.io/2017/7/26/85b12724868da664d2ba4f067bdd45a9?imageslim)

基于深度学习的标题添加系统,出现了错误识别


## 这个男孩正拿着一个棒球棒
特别地，这个是被强调的“对抗样例”，是被设计用于欺骗模型使它错误归类的。你已经注意到了，对输入空间扩充来产生能够最大化一些卷积网络滤波器（convnet filter）的输入，例如 —— 这是我们在第五章中介绍的滤波器可视化技术的基础（注：在 Deep Learning with Python中），还有第八章的 Deep Dream 算法。相似地，通过梯度增加，模型可以通过轻微地修改一幅照片来最大化给定的种类的预测空间。通过给熊猫照一张照片，并给它加入“长臂猿”的梯度，我们可以得到一个将这只熊猫归为长臂猿的神经网络。这证明了这些模型的脆弱之处，以及它们所进行的输入输出映射与人类意识的巨大不同。

![](https://user-gold-cdn.xitu.io/2017/7/26/d6af7fff2291f9e5887a42eeb0ad5a7b?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

一个对抗性的例子：图像中不可察觉的变化可以提升模型对图像的分类能力。

简而言之，深度学习模型一点也不理解它们的输入，至少从人类的角度来看（人类的理解）。我们对于图像、声音和语言的理解是基于我们人类的感觉——这是体现在全地球的生物身上的。机器学习模型是没有这方面的经验的，也因此无法以人类的方法“理解”它们得到的输入。通过标注大量的训练样例并代入训练模型，我们使得它们能够学习到几何变换，从而将这一集合中的例子映射到人类的概念之中，不过这个映射只是我们的意识中最简单最原始的草图，是从我们经验中的体现 —— 就如镜子中的黯淡影像一般。

![](https://user-gold-cdn.xitu.io/2017/7/26/30e79183685eea3b5e833ee5d32dd109?imageslim)

当前的学习模型：就如镜中暗淡的影子

作为一个机器学习的实践者，总是要注意这个，而且永远不要陷入陷阱，相信神经网络懂得它们所处理的任务 —— 它们并不懂，至少不是像我们一样理解。它们所被训练于的任务，范围远远窄于我们所希望真正教给它们的东西：仅仅是将训练的目标与输入映射，点对点。如果给它们展示任何偏离它们的训练数据集的东西，它们就会以极为荒谬的方式“坏掉”。
